{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of individual:  WHALEY DAVID A\n",
      "Number of NaN values in the corresponding feature set:  18\n",
      "Name of individual:  WROBEL BRUCE\n",
      "Number of NaN values in the corresponding feature set:  18\n",
      "Name of individual:  LOCKHART EUGENE E\n",
      "Number of NaN values in the corresponding feature set:  20\n",
      "Name of individual:  THE TRAVEL AGENCY IN THE PARK\n",
      "Number of NaN values in the corresponding feature set:  18\n",
      "Name of individual:  GRAMM WENDY L\n",
      "Number of NaN values in the corresponding feature set:  18\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"tools/\")\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester\n",
    "from tester import dump_classifier_and_data\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import numpy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "##################################################################################################################### \n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "financial_features = ['salary', \n",
    "                     'deferral_payments', \n",
    "                     'total_payments', \n",
    "                     'loan_advances', \n",
    "                     'bonus', \n",
    "                     'restricted_stock_deferred', \n",
    "                     'deferred_income', \n",
    "                     'total_stock_value', \n",
    "                     'expenses', \n",
    "                     'exercised_stock_options', \n",
    "                     'other', \n",
    "                     'long_term_incentive', \n",
    "                     'restricted_stock', \n",
    "                     'director_fees']\n",
    "\n",
    "email_features = ['to_messages', \n",
    "                  'from_poi_to_this_person', \n",
    "                  'from_messages', \n",
    "                  'from_this_person_to_poi', \n",
    "                  'shared_receipt_with_poi']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# pprint.pprint(data_dict)\n",
    "##################################################################################################################### \n",
    "### Task 2: Remove outliers\n",
    "\n",
    "# Finding employees with 18 or more NaNs in their feature set\n",
    "for i in data_dict:\n",
    "    count = 0\n",
    "    for j in data_dict[i]:\n",
    "        if data_dict[i][j] == 'NaN':\n",
    "            count += 1\n",
    "    if count > 17:\n",
    "        print 'Name of individual: ',i\n",
    "        print 'Number of NaN values in the corresponding feature set: ',count\n",
    "\n",
    "# not the name of a person\n",
    "data_dict.pop('TOTAL',0) \n",
    "# not the name of a person\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK',0) \n",
    "# all 20 features have NaN values\n",
    "data_dict.pop('LOCKHART EUGENE E',0) \n",
    "\n",
    "# Convert data from dictionary to dataframe\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index', dtype=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R score of predicting emails from poi to person in question using the salary:  0.0805558457316\n"
     ]
    }
   ],
   "source": [
    "# Remove NaN\n",
    "salary = df['salary'].replace(['NaN'], 0)\n",
    "from_poi = df['from_poi_to_this_person'].replace(['NaN'], 0)\n",
    "bonus = df['bonus'].replace(['NaN'], 0)\n",
    "\n",
    "# Re shape the data\n",
    "salary = numpy.reshape( numpy.array(salary), (len(salary), 1))\n",
    "from_poi = numpy.reshape( numpy.array(from_poi), (len(from_poi), 1))\n",
    "# bonus = numpy.reshape( numpy.array(bonus), (len(bonus), 1))\n",
    "\n",
    "# Split the data into training and testing sets to generate a regression line\n",
    "salary_train, salary_test, from_poi_train, from_poi_test = train_test_split(salary, \n",
    "                                                                            from_poi, \n",
    "                                                                            test_size=0.1, \n",
    "                                                                            random_state=42)\n",
    "\n",
    "# Using a regression line to view outliers\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg = reg.fit(salary_train, from_poi_train)\n",
    "reg_pred = reg.predict(salary_test)\n",
    "\n",
    "# Check accuracy of regression\n",
    "from sklearn.metrics import r2_score\n",
    "r = r2_score(from_poi_test, reg_pred)\n",
    "print 'R score of predicting emails from poi to person in question using the salary: ', r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI followed by the best features  ['poi', 'bonus', 'exercised_stock_options', 'net_worth', 'total_stock_value']\n"
     ]
    }
   ],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "my_df = pd.DataFrame.from_dict(my_dataset, orient='index', dtype=None)\n",
    "my_features_list = ['poi'] + financial_features + email_features\n",
    "\n",
    "# Clean the data to be used to set up new feature.\n",
    "salary = my_df['salary'].replace(['NaN'], 0,inplace = True)\n",
    "total_payments = my_df['total_payments'].replace(['NaN'], 0,inplace = True)\n",
    "bonus = my_df['bonus'].replace(['NaN'], 0,inplace = True)\n",
    "total_stock_value = my_df['total_stock_value'].replace(['NaN'], 0,inplace = True)\n",
    "\n",
    "# Create new column with total monetary assets -> ['net_worth'] using cleaned column data above.\n",
    "my_df['net_worth'] = my_df['salary'] + my_df['total_payments'] + my_df['bonus'] + my_df['total_stock_value']\n",
    "new_features_list = my_features_list + ['net_worth']\n",
    "\n",
    "# Convert dataframe back to dictionary\n",
    "my_dataset = my_df.to_dict(orient='index')\n",
    "\n",
    "\n",
    "### Extract features(email and financial) and labels(poi or not) from dataset for local testing\n",
    "\n",
    "# Takes a list of features ('features_list'), searches the data dictionary for those features, \n",
    "# and returns those features in the form of a data list.\n",
    "data = featureFormat(my_dataset, new_features_list, sort_keys = True)\n",
    "# Splits the data list, created by the previous statement, into poi(labels) and features\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Use feature selection to select k best features\n",
    "kbest = SelectKBest(k = 10)\n",
    "kbest.fit(features, labels)\n",
    "scores = kbest.scores_\n",
    "\n",
    "# Combine features with their scores\n",
    "features_scores = zip(new_features_list[1:], scores)\n",
    "\n",
    "# Top features\n",
    "features_scores = dict(features_scores[:21])\n",
    "sorted_features_scores = sorted(features_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "best_features = dict(sorted_features_scores[:4]).keys()\n",
    "best_features = ['poi'] + best_features\n",
    "\n",
    "# Scale the features                                                             \n",
    "#MinMax Scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "#print new_features_list\n",
    "print 'POI followed by the best features ', best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier details:  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy:  0.861488372093\n",
      "Precision:  0.129004761905\n",
      "Recall:  0.0464142857143\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "#     AdaBoostClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     SVC(gamma=5, C=2),\n",
    "#     LDA(),\n",
    "#     LogisticRegression(),\n",
    "    KNeighborsClassifier()\n",
    "#     KMeans(),\n",
    "#     DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "#     QDA()\n",
    "    ]\n",
    "\n",
    "for clf in classifiers:\n",
    "    accuracy, precision, recall = [], [], []\n",
    "    for i in range(500):\n",
    "        features = MinMaxScaler().fit_transform(features)\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, \n",
    "                                                                                    labels, \n",
    "                                                                                    test_size=0.3) \n",
    "        clf.fit(features_train, labels_train)\n",
    "        prediction = clf.predict(features_test)\n",
    "        # Append scores\n",
    "        accuracy.append(accuracy_score(labels_test, prediction))\n",
    "        precision.append(precision_score(labels_test, prediction, average=\"weighted\"))\n",
    "        recall.append(recall_score(labels_test, prediction, average=\"weighted\"))\n",
    "    print \"Classifier details: \", clf\n",
    "    print \"Accuracy: \", numpy.mean(accuracy)\n",
    "    print \"Precision: \", numpy.mean(precision)\n",
    "    print \"Recall: \", numpy.mean(recall)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
